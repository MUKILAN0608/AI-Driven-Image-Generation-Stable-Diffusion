{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_Dh8aIFF7rmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries in Colab\n",
        "!pip install diffusers transformers accelerate safetensors\n"
      ],
      "metadata": {
        "id": "XZ34f-Rj-yQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ“¦ Install Dependencies (if not installed)\n",
        "!pip install diffusers transformers accelerate safetensors\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "# ðŸ“¥ Imports\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ðŸ“Œ Device Setup\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ðŸ“¥ Load Stable Diffusion Pipeline\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16 if device==\"cuda\" else torch.float32\n",
        ").to(device)\n",
        "\n",
        "# ðŸ“¥ Load CLIP model for evaluation\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# ðŸ“Œ User Inputs\n",
        "prompt = input(\"Enter your prompt: \")\n",
        "guidance_scale = float(input(\"Enter guidance scale (5-15 recommended): \"))\n",
        "num_inference_steps = int(input(\"Enter number of inference steps (20-50 recommended): \"))\n",
        "num_images = int(input(\"How many images to generate for this prompt? \"))\n",
        "\n",
        "# ðŸ“¦ Image Generation + CLIP Scoring\n",
        "images = []\n",
        "clip_scores = []\n",
        "\n",
        "for i in tqdm(range(num_images)):\n",
        "    with torch.autocast(device):\n",
        "        image = pipe(prompt, guidance_scale=guidance_scale, num_inference_steps=num_inference_steps).images[0]\n",
        "    images.append(image)\n",
        "\n",
        "    # Compute CLIP Score\n",
        "    inputs = clip_processor(text=prompt, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
        "    outputs = clip_model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image\n",
        "    score = logits_per_image.item()\n",
        "    clip_scores.append(score)\n",
        "\n",
        "    display(image)\n",
        "    print(f\"Image {i+1} CLIP score: {score:.4f}\")\n",
        "\n",
        "# ðŸ“Š Average CLIP Score\n",
        "print(f\"\\nAverage CLIP Score for prompt '{prompt}': {np.mean(clip_scores):.4f}\")\n",
        "\n",
        "# ðŸ“Œ Optional: Latent Space Interpolation (between two prompts)\n",
        "def interpolate_images(prompt1, prompt2, num_steps=5):\n",
        "    print(f\"\\nInterpolating between:\\n'{prompt1}' â†” '{prompt2}'\")\n",
        "    latent_images = []\n",
        "    with torch.no_grad(), torch.autocast(device):\n",
        "        latents1 = pipe(prompt1, output_type=\"latent\", num_inference_steps=num_inference_steps, guidance_scale=guidance_scale).images[0]\n",
        "        latents2 = pipe(prompt2, output_type=\"latent\", num_inference_steps=num_inference_steps, guidance_scale=guidance_scale).images[0]\n",
        "\n",
        "        for i in range(num_steps):\n",
        "            alpha = i / (num_steps - 1)\n",
        "            latent = (1 - alpha) * latents1 + alpha * latents2\n",
        "            latent_image = pipe.decode_latents(latent.unsqueeze(0))[0]\n",
        "            latent_image_pil = pipe.numpy_to_pil(latent_image)[0]\n",
        "            latent_images.append(latent_image_pil)\n",
        "            display(latent_image_pil)\n",
        "            print(f\"Interpolation Step {i+1}/{num_steps} | Î± = {alpha:.2f}\")\n",
        "\n",
        "    return latent_images\n",
        "\n",
        "# Uncomment to run interpolation between two prompts:\n",
        "# interpolate_images(\"A futuristic cyberpunk city\", \"A medieval castle at sunset\", num_steps=6)\n"
      ],
      "metadata": {
        "id": "12qagvWTAClV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EeyJjlreJ4mr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}